# -*- coding: utf-8 -*-
"""
tools/run_phase15_realdata.py
Phase 1.5 ì‹¤ë°ì´í„° ë£¨í”„ - Gate2ê¹Œì§€ ë°˜ë³µ ì‹¤í–‰

ì‹¤í–‰: python -m tools.run_phase15_realdata [--runs 3] [--trials 30] [--seed 42]

ëª©ì :
- Run A/B/C (ê¸°ê°„/ìœ ë‹ˆë²„ìŠ¤ë§Œ ë‹¤ë¥´ê²Œ)ë¡œ Gate2ê¹Œì§€ ì‹¤í–‰
- ê° Run ê²°ê³¼ë¡œ 4ê°œ ì²´í¬ë¥¼ ì¶œë ¥:
  1. Gate1 ì™„ë£Œ
  2. Gate2(WF outsample) ì™„ë£Œ
  3. ë£©ë°±ë³„ cache_key[:8]ê°€ ì„œë¡œ ë‹¤ë¦„
  4. manifestì— universe_hash/data_version/lookback debugê°€ ì¡´ì¬
- ìµœì¢… ìš”ì•½ì—ì„œ "3íšŒ ëª¨ë‘ í†µê³¼/ì‹¤íŒ¨"ë¥¼ í•œ ì¤„ë¡œ ì¶œë ¥
"""
import argparse
import json
import logging
import sys
from datetime import date
from pathlib import Path
from typing import Dict, List, Any

sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import optuna

    optuna.logging.set_verbosity(optuna.logging.WARNING)
except ImportError:
    print("âŒ optuna íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install optuna")
    sys.exit(1)

from extensions.tuning import (
    SplitConfig,
    DataConfig,
    DEFAULT_COSTS,
    TuningObjective,
    MiniWalkForward,
    check_gate1,
    check_gate2,
    deduplicate_top_n_candidates,
    run_backtest_for_tuning,
    clear_global_cache,
    create_manifest,
    save_manifest,
    compute_universe_hash,
)
from extensions.tuning.gates import is_test_mode
from extensions.tuning.telemetry import (
    init_telemetry,
    get_telemetry,
    emit_run_start,
    emit_run_end,
    emit_run_config,
    emit_data_preflight,
    emit_gate1_summary,
    emit_gate2_summary,
    emit_guardrail_distribution,
    emit_manifest_saved,
    emit_error,
    EventStage,
)
from app.services.data_preflight import run_preflight

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger(__name__)


# ============================================================
# ìœ ë‹ˆë²„ìŠ¤ ì •ì˜
# ============================================================
UNIVERSES = {
    "A": {
        "name": "ETF ëŒ€í˜• 10ì¢…",
        "tickers": [
            "069500",
            "102110",
            "229200",
            "114800",
            "122630",
            "233740",
            "252670",
            "261240",
            "305720",
            "091160",
        ],
    },
    "B": {
        "name": "ETF ì¤‘í˜• 10ì¢… (ìƒ˜í”Œ)",
        "tickers": [
            "000660",
            "005930",
            "035420",
            "035720",
            "051910",
            "069500",
            "091160",
            "102110",
            "114800",
            "122630",
        ],
    },
    "C": {
        "name": "ETF í˜¼í•© 10ì¢… (ìƒ˜í”Œ)",
        "tickers": [
            "229200",
            "233740",
            "252670",
            "261240",
            "305720",
            "000660",
            "005930",
            "035420",
            "035720",
            "051910",
        ],
    },
}

PERIODS = {
    "A": {"start": date(2020, 1, 1), "end": date(2024, 6, 30)},
    "B": {"start": date(2019, 1, 1), "end": date(2024, 6, 30)},
    "C": {"start": date(2018, 1, 1), "end": date(2024, 6, 30)},
}


def get_trading_calendar(start: date, end: date) -> List[date]:
    """ê±°ë˜ì¼ ìº˜ë¦°ë” ìƒì„± (ì£¼ë§ ì œì™¸)"""
    from datetime import timedelta

    calendar = []
    current = start
    while current <= end:
        if current.weekday() < 5:
            calendar.append(current)
        current += timedelta(days=1)
    return calendar


def _save_empty_manifest(
    result: Dict[str, Any],
    start_date: date,
    end_date: date,
    lookbacks: List[int],
    n_trials: int,
    split_config,
    data_config,
    param_ranges: Dict[str, Any],
    seed: int,
) -> None:
    """Gate1 ì‹¤íŒ¨ ì‹œì—ë„ manifest ì €ì¥í•˜ì—¬ data_version ê²€ì¦ ê°€ëŠ¥í•˜ë„ë¡"""
    from pathlib import Path

    manifest = create_manifest(
        stage="gate1_failed",
        start_date=start_date,
        end_date=end_date,
        lookbacks=lookbacks,
        trials=n_trials,
        split_config=split_config,
        costs=DEFAULT_COSTS,
        data_config=data_config,
        param_ranges=param_ranges,
        best_result=None,
        all_trials_count=n_trials,
        random_seed=seed,
    )

    output_dir = Path(__file__).parent.parent / "data" / "tuning_test"
    filepath = save_manifest(manifest, output_dir)
    result["manifest_path"] = str(filepath)

    # Manifest ê²€ì¦
    manifest_dict = manifest.to_dict()
    has_universe_hash = bool(manifest_dict.get("data", {}).get("universe_hash"))
    has_data_version = bool(manifest_dict.get("data", {}).get("data_version"))
    data_version = manifest_dict.get("data", {}).get("data_version", "")

    if has_universe_hash and has_data_version:
        result["checks"]["manifest_valid"] = True
        print(f"    âœ… Manifest ì €ì¥ (Gate1 ì‹¤íŒ¨)")
        print(f"       - data_version: {data_version}")
        print(
            f"       - universe_hash: {manifest_dict['data']['universe_hash'][:16]}..."
        )
    else:
        print(f"    âŒ Manifest ê²€ì¦ ì‹¤íŒ¨")

    print(f"    ì €ì¥: {filepath.name}")


def _save_failed_manifest(
    result: Dict[str, Any],
    start_date: date,
    end_date: date,
    lookbacks: List[int],
    n_trials: int,
    split_config,
    data_config,
    param_ranges: Dict[str, Any],
    seed: int,
    error_type: str,
    error_details: List[str],
) -> None:
    """ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë“± ì—ëŸ¬ ì‹œ manifest ì €ì¥ (ì‹¤íŒ¨ ì›ì¸ ê¸°ë¡)"""
    from pathlib import Path

    # stageë¥¼ error_typeìœ¼ë¡œ ì„¤ì •
    stage = f"failed_{error_type.lower()}"

    manifest = create_manifest(
        stage=stage,
        start_date=start_date,
        end_date=end_date,
        lookbacks=lookbacks,
        trials=n_trials,
        split_config=split_config,
        costs=DEFAULT_COSTS,
        data_config=data_config,
        param_ranges=param_ranges,
        best_result=None,
        all_trials_count=0,
        random_seed=seed,
    )

    output_dir = Path(__file__).parent.parent / "data" / "tuning_test"
    filepath = save_manifest(manifest, output_dir)
    result["manifest_path"] = str(filepath)

    # Manifest ê²€ì¦
    manifest_dict = manifest.to_dict()
    data_version = manifest_dict.get("data", {}).get("data_version", "")

    result["checks"]["manifest_valid"] = True
    print(f"    âœ… Manifest ì €ì¥ ({error_type})")
    print(f"       - data_version: {data_version}")
    print(f"       - error_type: {error_type}")
    print(f"       - error_count: {len(error_details)}")
    print(f"    ì €ì¥: {filepath.name}")


def run_single_phase15(
    run_id: str,
    universe: Dict[str, Any],
    period: Dict[str, date],
    n_trials: int,
    seed: int,
    top_n: int,
    use_mock: bool = True,
    analysis_mode: bool = False,
    force_gate2: bool = False,
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ Phase 1.5 ì‹¤í–‰

    Args:
        analysis_mode: Trueë©´ ê°€ë“œë ˆì¼ ì‹¤íŒ¨í•´ë„ manifest ì €ì¥ ë° ì‹¤íŒ¨ ì‚¬ìœ  ì§‘ê³„
        force_gate2: Trueë©´ Gate1 í›„ë³´ê°€ 0ì¼ ë•Œ ê°€ë“œë ˆì¼ ë¬´ì‹œí•˜ê³  Top-Nì„ ë½‘ì•„ Gate2 ì‹¤í–‰
                     (analysis_modeì—ì„œë§Œ í—ˆìš©, Gate3ëŠ” ì ˆëŒ€ ì‹¤í–‰ ë¶ˆê°€)

    Returns:
        ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (checks, manifest_path, failure_reasons ë“±)
    """
    result = {
        "run_id": run_id,
        "universe_name": universe["name"],
        "period": f"{period['start']}~{period['end']}",
        "use_mock": use_mock,
        "checks": {
            "gate1_complete": False,
            "gate2_complete": False,
            "cache_keys_distinct": False,
            "manifest_valid": False,
            "preflight_ok": False,
        },
        "failure_reasons": {},  # ê°€ë“œë ˆì¼ ì‹¤íŒ¨ ì‚¬ìœ  ì§‘ê³„
        "gate1_passed": 0,
        "gate2_passed": 0,
        "gate2_ran": False,  # WFê°€ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
        "gate1_relaxed": False,  # force_gate2ë¡œ ê°€ë“œë ˆì¼ ì™„í™” ì—¬ë¶€
        "manifest_path": None,
        "telemetry_path": None,
        "cache_keys": {},
        "preflight_failures": [],
    }

    # í…”ë ˆë©”íŠ¸ë¦¬ ì´ˆê¸°í™”
    telemetry_run_id = f"phase15_{run_id}_{date.today().strftime('%Y%m%d')}_{seed}"
    init_telemetry(telemetry_run_id)

    print(f"\n{'=' * 60}")
    print(f"Run {run_id}: {universe['name']}")
    print(f"ê¸°ê°„: {period['start']} ~ {period['end']}")
    print(f"ìœ ë‹ˆë²„ìŠ¤: {len(universe['tickers'])}ì¢…ëª©")
    print(f"{'=' * 60}")

    # Mock íŒ¨ì¹˜
    if use_mock:
        import extensions.tuning.runner as runner_module
        from tests.tuning.test_mini_tuning import (
            mock_run_single_backtest,
            MockBacktestService,
        )
        import tests.tuning.test_mini_tuning as mini_tuning_module

        original_func = runner_module._run_single_backtest
        runner_module._run_single_backtest = mock_run_single_backtest
        mini_tuning_module._mock_service = MockBacktestService(seed)

    clear_global_cache()

    try:
        # ì„¤ì •
        start_date = period["start"]
        end_date = period["end"]
        trading_calendar = get_trading_calendar(start_date, end_date)

        universe_hash = compute_universe_hash(universe["tickers"])

        lookbacks = [3, 6, 12]
        # stop_loss_pct: ì–‘ìˆ˜ ì†Œìˆ˜ (0.03~0.10 = 3%~10%)
        # unit: "decimal_positive" (ì˜ˆ: 0.05 = 5% ì†ì ˆ)
        param_ranges = {
            "ma_period": {"min": 20, "max": 100, "step": 10, "type": "int"},
            "rsi_period": {"min": 5, "max": 25, "step": 5, "type": "int"},
            "stop_loss_pct": {
                "min": 0.03,
                "max": 0.10,
                "step": 0.01,
                "type": "float",
                "unit": "decimal_positive",
            },
        }

        split_config = SplitConfig()
        data_config = DataConfig(
            data_version="mock_v1" if use_mock else "real_v1",
            universe_version=f"etf_{run_id.lower()}_v1",
            universe_hash=universe_hash,
            universe_count=len(universe["tickers"]),
            sample_codes=universe["tickers"][:5],
        )

        # RUN_START ì´ë²¤íŠ¸
        emit_run_start(
            EventStage.PHASE15.value,
            run_id,
            {
                "n_trials": n_trials,
                "seed": seed,
                "top_n": top_n,
                "use_mock": use_mock,
                "analysis_mode": analysis_mode,
                "universe_count": len(universe["tickers"]),
                "period": f"{start_date}~{end_date}",
            },
        )

        # RUN_CONFIG ì´ë²¤íŠ¸ - ì‹¤í–‰ ì„¤ì • ê¸°ë¡ (real/mock í™•ì¸ìš©)
        emit_run_config(
            stage=EventStage.PHASE15.value,
            use_mock=use_mock,
            test_mode=is_test_mode(),
            analysis_mode=analysis_mode,
            skip_logic_check=False,
            skip_mdd_check=False,
            data_version=data_config.data_version,
            requested_hash=universe_hash,
            effective_hash=universe_hash,  # ì‹¤ì œë¡œëŠ” preflight í›„ ê³„ì‚°
            period_start=start_date,
            period_end=end_date,
            wf_preset="default",
        )

        # ============================================================
        # Phase 0: Preflight (ì‹¤ë°ì´í„° ì‚¬ì „ê²€ì¦)
        # ============================================================
        if not use_mock:
            print(f"\n  [Phase 0] Preflight (ì‹¤ë°ì´í„° ì‚¬ì „ê²€ì¦)")
            preflight_report = run_preflight(
                universe_codes=universe["tickers"],
                start_date=start_date,
                end_date=end_date,
                data_version=data_config.data_version,
            )

            # í…”ë ˆë©”íŠ¸ë¦¬ ê¸°ë¡
            emit_data_preflight(
                EventStage.PHASE15.value,
                preflight_report.ok,
                preflight_report.fail_count,
                preflight_report.failures,
                preflight_report.sample_stats,
            )

            result["checks"]["preflight_ok"] = preflight_report.ok
            result["preflight_failures"] = preflight_report.failures
            result["preflight_pass_ratio"] = preflight_report.pass_ratio

            # Phase 2.1: data_digest ë° ê³µí†µ ê¸°ê°„ ì €ì¥
            result["data_digest"] = preflight_report.data_digest
            result["common_period_start"] = (
                preflight_report.common_period_start.isoformat()
                if preflight_report.common_period_start
                else None
            )
            result["common_period_end"] = (
                preflight_report.common_period_end.isoformat()
                if preflight_report.common_period_end
                else None
            )

            if preflight_report.ok:
                print(
                    f"    âœ… Preflight í†µê³¼: {preflight_report.pass_count}/{preflight_report.total_count}"
                )
                print(f"    data_digest: {preflight_report.data_digest}")
                if preflight_report.common_period_start and preflight_report.common_period_end:
                    print(
                        f"    common_period: {preflight_report.common_period_start} ~ "
                        f"{preflight_report.common_period_end}"
                    )
            elif preflight_report.pass_ratio >= 0.8:
                # pass_ratio >= 0.8ì´ë©´ PASSí•œ í‹°ì»¤ë§Œìœ¼ë¡œ ì¶•ì†Œ universe êµ¬ì„±
                passed_tickers = [
                    ticker
                    for ticker, r in preflight_report.ticker_results.items()
                    if r.ok
                ]
                print(
                    f"    âš ï¸ Preflight ë¶€ë¶„ í†µê³¼: {preflight_report.pass_count}/{preflight_report.total_count} "
                    f"(pass_ratio={preflight_report.pass_ratio:.1%} >= 80%)"
                )
                print(f"    â†’ ì¶•ì†Œ universeë¡œ ì§„í–‰: {len(passed_tickers)}ê°œ í‹°ì»¤")
                for failure in preflight_report.failures[:3]:
                    print(f"       ì œì™¸: {failure}")

                # universeë¥¼ ì¶•ì†Œ
                universe["tickers"] = passed_tickers
                universe["original_count"] = preflight_report.total_count
                universe["reduced"] = True
                result["checks"]["preflight_ok"] = True  # ì¶•ì†Œ universeë¡œ ì§„í–‰
                result["reduced_universe"] = True
            else:
                print(
                    f"    âŒ Preflight ì‹¤íŒ¨: {preflight_report.fail_count}/{preflight_report.total_count} "
                    f"(pass_ratio={preflight_report.pass_ratio:.1%} < 80%)"
                )
                for failure in preflight_report.failures[:5]:
                    print(f"       - {failure}")

                # ì‹¤íŒ¨ ì‹œ FAILED_PREFLIGHTë¡œ ì¢…ë£Œ
                emit_error(
                    EventStage.PHASE15.value,
                    "PREFLIGHT_FAILED",
                    f"{preflight_report.fail_count}ê°œ í‹°ì»¤ ê²€ì¦ ì‹¤íŒ¨ (pass_ratio={preflight_report.pass_ratio:.1%} < 80%)",
                    {"failures": preflight_report.failures},
                )

                # analysis_modeë©´ ì‹¤íŒ¨ manifest ì €ì¥
                if analysis_mode:
                    result["data_load_error"] = {
                        "status": "FAILED_PREFLIGHT",
                        "fail_count": preflight_report.fail_count,
                        "pass_ratio": preflight_report.pass_ratio,
                        "failures": preflight_report.failures[:10],
                    }
                    _save_failed_manifest(
                        result,
                        start_date,
                        end_date,
                        lookbacks,
                        n_trials,
                        split_config,
                        data_config,
                        param_ranges,
                        seed,
                        error_type="PREFLIGHT_FAILED",
                        error_details=preflight_report.failures,
                    )

                emit_run_end(
                    EventStage.PHASE15.value,
                    run_id,
                    False,
                    {
                        "status": "FAILED_PREFLIGHT",
                        "failures": preflight_report.failures,
                    },
                )
                return result
        else:
            result["checks"]["preflight_ok"] = True  # Mock ëª¨ë“œëŠ” preflight ìŠ¤í‚µ

        # ============================================================
        # Phase 1: íŠœë‹ ì‹¤í–‰
        # ============================================================
        print(f"\n  [Phase 1] íŠœë‹ ì‹¤í–‰ (n_trials={n_trials})")

        objective = TuningObjective(
            start_date=start_date,
            end_date=end_date,
            trading_calendar=trading_calendar,
            lookbacks=lookbacks,
            param_ranges=param_ranges,
            split_config=split_config,
            data_config=data_config,
            universe_codes=universe["tickers"],
        )

        sampler = optuna.samplers.TPESampler(seed=seed)
        study = optuna.create_study(direction="maximize", sampler=sampler)
        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)

        stats = objective.get_stats()
        print(
            f"    ì™„ë£Œ: {stats['trial_count']}ê±´, ê°€ë“œë ˆì¼ ì‹¤íŒ¨: {stats['guardrail_failures']}ê±´"
        )

        # ì‹¤íŒ¨ ì‚¬ìœ  ì§‘ê³„ (histogram í˜•íƒœ)
        result["failure_reasons"] = stats.get("guardrail_fail_reasons", {})
        failure_reasons = stats.get("guardrail_fail_reasons", {})
        fail_reason_pct = stats.get("guardrail_fail_reason_pct", {})
        sorted_reasons = sorted(
            failure_reasons.items(), key=lambda x: x[1], reverse=True
        )
        top5_reasons = sorted_reasons[:5]

        if failure_reasons:
            total_failures = sum(failure_reasons.values())
            print(f"    ê°€ë“œë ˆì¼ ì‹¤íŒ¨ ì‚¬ìœ  íˆìŠ¤í† ê·¸ë¨ (ì´ {total_failures}ê±´):")
            for code, count in sorted_reasons:
                pct = fail_reason_pct.get(code, 0) * 100
                bar_len = int(pct / 5)  # 5% = 1ì¹¸
                bar = "â–ˆ" * bar_len
                print(f"      {code:<15} {bar:<20} {pct:5.1f}% ({count}ê±´)")

        # GUARDRAIL_DISTRIBUTION ì´ë²¤íŠ¸
        emit_guardrail_distribution(
            EventStage.PHASE15.value,
            stats["trial_count"],
            failure_reasons,
            [{"code": code, "count": count} for code, count in top5_reasons],
        )

        # ============================================================
        # Phase 2: Gate 1 - Top-N ì„ ì •
        # ============================================================
        print(f"\n  [Phase 2] Gate 1 - Top-N ì„ ì •")

        completed_trials = [
            t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE
        ]

        candidates = []
        for trial in completed_trials:
            if "val_sharpe" in trial.user_attrs:
                candidates.append(
                    {
                        "trial_number": trial.number,
                        "params": trial.params,
                        "val_sharpe": trial.user_attrs["val_sharpe"],
                        "params_hash": trial.user_attrs.get("params_hash", ""),
                    }
                )

        deduped_candidates = deduplicate_top_n_candidates(candidates, top_n=top_n)
        dedup_removed = len(candidates) - len(deduped_candidates) - (top_n - len(deduped_candidates)) if len(candidates) > top_n else 0
        print(f"    candidates={len(candidates)}, selected_top_n={len(deduped_candidates)}, dedup_removed={max(0, dedup_removed)}")

        # ë£©ë°±ë³„ cache_key ìˆ˜ì§‘
        cache_keys_by_lookback = {}

        gate1_passed = []
        for c in deduped_candidates:
            # ë©€í‹° ë£©ë°± ì¦ê±° ìˆ˜ì§‘ (by_lookback)
            by_lookback = {}
            scores_by_lb = {}

            for lb in lookbacks:
                bt_result = run_backtest_for_tuning(
                    params=c["params"],
                    start_date=start_date,
                    end_date=end_date,
                    lookback_months=lb,
                    trading_calendar=trading_calendar,
                    use_cache=True,
                    universe_codes=universe["tickers"],
                )
                if bt_result.debug and bt_result.debug.cache_key:
                    cache_keys_by_lookback[lb] = bt_result.debug.cache_key[:8]

                # by_lookback ë°ì´í„° ìˆ˜ì§‘
                val = bt_result.val
                if val:
                    score = val.sharpe - 0.5 * abs(
                        val.mdd
                    )  # val_sharpe_with_mdd_penalty

                    # Phase 2.1: ë©€í‹°ë£©ë°± ì¦ê±° ê°•í™” - debug í•„ë“œ ì¶”ê°€
                    debug_fields = {}
                    if bt_result.debug:
                        debug_fields = {
                            "effective_eval_start": (
                                bt_result.debug.effective_eval_start.isoformat()
                                if bt_result.debug.effective_eval_start
                                else None
                            ),
                            "bars_used": bt_result.debug.bars_used,
                            "signal_days": bt_result.debug.signal_days,
                            "order_count": bt_result.debug.order_count,
                            "lookback_start_date": (
                                bt_result.debug.lookback_start_date.isoformat()
                                if bt_result.debug.lookback_start_date
                                else None
                            ),
                        }

                    by_lookback[lb] = {
                        "val_sharpe": val.sharpe,
                        "val_cagr": val.cagr,
                        "val_mdd": val.mdd,
                        "score": score,
                        "cache_key": (
                            bt_result.debug.cache_key[:8] if bt_result.debug else ""
                        ),
                        "debug": debug_fields,  # Phase 2.1 ì¶”ê°€
                    }
                    scores_by_lb[lb] = score

            # combined_score ê³„ì‚° (min ê²°í•©)
            combined_score = min(scores_by_lb.values()) if scores_by_lb else -999.0
            min_lookback_months = (
                min(scores_by_lb, key=scores_by_lb.get) if scores_by_lb else 12
            )

            # Gate1 ì²´í¬ (min_lookback_months ê¸°ì¤€)
            bt_result = run_backtest_for_tuning(
                params=c["params"],
                start_date=start_date,
                end_date=end_date,
                lookback_months=min_lookback_months,
                trading_calendar=trading_calendar,
                use_cache=True,
                universe_codes=universe["tickers"],
            )

            gate1_result = check_gate1(
                bt_result,
                top_n=top_n,
                skip_logic_check=False,
                skip_mdd_check=False,
            )

            if gate1_result.passed:
                gate1_passed.append(
                    {
                        **c,
                        "result": bt_result,
                        "gate1_result": gate1_result,
                        "by_lookback": by_lookback,
                        "combined_score": combined_score,
                        "min_lookback_months": min_lookback_months,
                    }
                )
                params_json = json.dumps(c["params"], sort_keys=True)
                print(
                    f"    âœ… Trial #{c['trial_number']}: Val Sharpe={c['val_sharpe']:.3f}"
                )
                print(f"       params: {params_json}")
            else:
                print(f"    âŒ Trial #{c['trial_number']}: {gate1_result.failures[:2]}")

        result["gate1_passed"] = len(gate1_passed)
        result["cache_keys"] = cache_keys_by_lookback

        # cache_key êµ¬ë¶„ í™•ì¸
        if len(set(cache_keys_by_lookback.values())) == len(lookbacks):
            result["checks"]["cache_keys_distinct"] = True
            print(f"    âœ… ë£©ë°±ë³„ cache_key êµ¬ë¶„: {cache_keys_by_lookback}")
        else:
            print(f"    âŒ ë£©ë°±ë³„ cache_key ë™ì¼: {cache_keys_by_lookback}")

        # GATE1_SUMMARY ì´ë²¤íŠ¸
        emit_gate1_summary(
            EventStage.GATE1.value,
            len(completed_trials),
            len(gate1_passed),
            top_n,
            failure_reasons,
        )

        if gate1_passed:
            result["checks"]["gate1_complete"] = True
            print(f"    Gate 1 í†µê³¼: {len(gate1_passed)}/{len(deduped_candidates)}")
        elif force_gate2 and analysis_mode and (deduped_candidates or completed_trials):
            # force_gate2: ê°€ë“œë ˆì¼ ë¬´ì‹œí•˜ê³  Top-Nì„ ë½‘ì•„ Gate2 ì‹¤í–‰
            # (analysis_modeì—ì„œë§Œ í—ˆìš©, Gate3ëŠ” ì ˆëŒ€ ì‹¤í–‰ ë¶ˆê°€)
            print(f"    âš ï¸ Gate 1 í†µê³¼ í›„ë³´ ì—†ìŒ â†’ force_gate2 ëª¨ë“œë¡œ ê°€ë“œë ˆì¼ ì™„í™”")
            result["gate1_relaxed"] = True

            # deduped_candidatesê°€ ë¹„ì–´ìˆìœ¼ë©´ completed_trialsì—ì„œ ì§ì ‘ ì¶”ì¶œ
            if deduped_candidates:
                source_candidates = deduped_candidates
            else:
                # completed_trialsì—ì„œ params ì¶”ì¶œ
                source_candidates = []
                for trial in completed_trials[:top_n]:
                    source_candidates.append({
                        "trial_number": trial.number,
                        "params": trial.params,
                        "val_sharpe": trial.user_attrs.get("val_sharpe", 0),
                        "params_hash": trial.user_attrs.get("params_hash", ""),
                    })

            # ê°€ë“œë ˆì¼ ë¬´ì‹œí•˜ê³  val_sharpe ê¸°ì¤€ Top-N ì„ ì •
            sorted_candidates = sorted(
                source_candidates, key=lambda x: x["val_sharpe"], reverse=True
            )[:top_n]

            for c in sorted_candidates:
                # ë©€í‹° ë£©ë°± ì¦ê±° ìˆ˜ì§‘ (by_lookback)
                by_lookback = {}
                scores_by_lb = {}

                for lb in lookbacks:
                    bt_result = run_backtest_for_tuning(
                        params=c["params"],
                        start_date=start_date,
                        end_date=end_date,
                        lookback_months=lb,
                        trading_calendar=trading_calendar,
                        use_cache=True,
                        universe_codes=universe["tickers"],
                    )

                    val = bt_result.val
                    if val:
                        score = val.sharpe - 0.5 * abs(val.mdd)
                        by_lookback[lb] = {
                            "val_sharpe": val.sharpe,
                            "val_cagr": val.cagr,
                            "val_mdd": val.mdd,
                            "score": score,
                            "cache_key": (
                                bt_result.debug.cache_key[:8] if bt_result.debug else ""
                            ),
                        }
                        scores_by_lb[lb] = score

                combined_score = min(scores_by_lb.values()) if scores_by_lb else -999.0
                min_lookback_months = (
                    min(scores_by_lb, key=scores_by_lb.get) if scores_by_lb else 12
                )

                # Gate1 ì²´í¬ ì—†ì´ ë°”ë¡œ ì¶”ê°€ (ê°€ë“œë ˆì¼ ì™„í™”)
                bt_result = run_backtest_for_tuning(
                    params=c["params"],
                    start_date=start_date,
                    end_date=end_date,
                    lookback_months=min_lookback_months,
                    trading_calendar=trading_calendar,
                    use_cache=True,
                    universe_codes=universe["tickers"],
                )

                gate1_passed.append(
                    {
                        **c,
                        "result": bt_result,
                        "gate1_result": None,  # ê°€ë“œë ˆì¼ ì™„í™”ë¡œ Gate1 ê²°ê³¼ ì—†ìŒ
                        "by_lookback": by_lookback,
                        "combined_score": combined_score,
                        "min_lookback_months": min_lookback_months,
                        "guardrail_relaxed": True,
                    }
                )
                print(f"    âš ï¸ Trial #{c['trial_number']}: Val Sharpe={c['val_sharpe']:.3f} (ê°€ë“œë ˆì¼ ì™„í™”)")

            result["gate1_passed"] = len(gate1_passed)
            print(f"    Gate 1 ì™„í™” í†µê³¼: {len(gate1_passed)}ê°œ (guardrail_relaxed=True)")
        else:
            print(f"    âš ï¸ Gate 1 í†µê³¼ í›„ë³´ ì—†ìŒ")
            # Gate1 ì‹¤íŒ¨í•´ë„ manifest ê²€ì¦ì„ ìœ„í•´ ë¹ˆ manifest ìƒì„±
            _save_empty_manifest(
                result,
                start_date,
                end_date,
                lookbacks,
                n_trials,
                split_config,
                data_config,
                param_ranges,
                seed,
            )
            return result

        # ============================================================
        # Phase 3: Gate 2 - WF Outsample ì•ˆì •ì„±
        # ============================================================
        print(f"\n  [Phase 3] Gate 2 - WF Outsample ì•ˆì •ì„±")

        gate2_passed = []
        for c in gate1_passed:
            wf = MiniWalkForward(
                start_date=start_date,
                end_date=end_date,
                trading_calendar=trading_calendar,
                train_months=12,
                val_months=3,
                outsample_months=3,
                stride_months=6,
                universe_codes=universe["tickers"],
            )

            wf_results_list = wf.run(c["params"])
            wf_results = [
                {"sharpe": r.outsample_metrics.sharpe if r.outsample_metrics else 0}
                for r in wf_results_list
            ]

            # WF outsample sharpe ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            wf_outsample_sharpes = [r["sharpe"] for r in wf_results]

            gate2_result = check_gate2(
                c["result"],
                wf_results,
                min_stability_score=1.0,
                min_win_rate=0.60,
            )

            stability = gate2_result.metadata.get("stability_score", 0)
            win_rate = gate2_result.metadata.get("win_rate", 0)

            if gate2_result.passed:
                gate2_passed.append(
                    {
                        **c,
                        "gate2_result": gate2_result,
                        "stability_score": stability,
                        "win_rate": win_rate,
                        "wf_windows": len(wf_results_list),
                        "wf_outsample_sharpes": wf_outsample_sharpes,
                    }
                )
                print(
                    f"    âœ… Trial #{c['trial_number']}: stability={stability:.2f}, win_rate={win_rate:.0%}"
                )
            else:
                print(
                    f"    âŒ Trial #{c['trial_number']}: stability={stability:.2f}, win_rate={win_rate:.0%}"
                )

        result["gate2_passed"] = len(gate2_passed)
        result["gate2_ran"] = True  # WFê°€ ì‹¤í–‰ë¨

        # GATE2_SUMMARY ì´ë²¤íŠ¸
        best_stability = max((g["stability_score"] for g in gate2_passed), default=0.0)
        emit_gate2_summary(
            EventStage.GATE2.value,
            len(gate1_passed),
            len(gate2_passed),
            wf_windows=6,  # MiniWalkForward ê¸°ë³¸ ìœˆë„ìš° ìˆ˜
            best_stability_score=best_stability,
        )

        if gate2_passed:
            result["checks"]["gate2_complete"] = True
            print(f"    Gate 2 í†µê³¼: {len(gate2_passed)}/{len(gate1_passed)}")
        else:
            print(f"    âš ï¸ Gate 2 í†µê³¼ í›„ë³´ ì—†ìŒ")
            return result

        # ============================================================
        # Phase 4: Manifest ì €ì¥
        # ============================================================
        print(f"\n  [Phase 4] Manifest ì €ì¥")

        best_candidate = max(gate2_passed, key=lambda x: x["stability_score"])

        # analysis_modeë©´ stageë¥¼ "analysis"ë¡œ ê°•ì œ
        manifest_stage = "analysis" if analysis_mode else "gate2"
        manifest = create_manifest(
            stage=manifest_stage,
            start_date=start_date,
            end_date=end_date,
            lookbacks=lookbacks,
            trials=n_trials,
            split_config=split_config,
            costs=DEFAULT_COSTS,
            data_config=data_config,
            param_ranges=param_ranges,
            best_result=best_candidate["result"],
            all_trials_count=len(study.trials),
            random_seed=seed,
            # ë©€í‹° ë£©ë°± ì¦ê±° (v4.1)
            by_lookback=best_candidate.get("by_lookback"),
            combined_score=best_candidate.get("combined_score"),
            min_lookback_months=best_candidate.get("min_lookback_months"),
            # Gate2 WF ê²°ê³¼ (v4.1)
            wf_windows=best_candidate.get("wf_windows"),
            wf_outsample_sharpes=best_candidate.get("wf_outsample_sharpes"),
            wf_stability_score=best_candidate.get("stability_score"),
            wf_win_rate=best_candidate.get("win_rate"),
        )

        output_dir = Path(__file__).parent.parent / "data" / "tuning_test"
        filepath = save_manifest(manifest, output_dir)
        result["manifest_path"] = str(filepath)

        # MANIFEST_SAVED ì´ë²¤íŠ¸
        emit_manifest_saved(
            EventStage.PHASE15.value,
            str(filepath),
            manifest_stage,
            data_config.data_version,
            data_config.universe_hash,
        )

        # Manifest ê²€ì¦
        manifest_dict = manifest.to_dict()
        has_universe_hash = bool(manifest_dict.get("data", {}).get("universe_hash"))
        has_data_version = bool(manifest_dict.get("data", {}).get("data_version"))
        has_debug = bool(
            manifest_dict.get("results", {}).get("best_trial", {}).get("debug")
        )

        # ë©€í‹° ë£©ë°± ì¦ê±° ê²€ì¦
        best_trial = manifest_dict.get("results", {}).get("best_trial", {})
        has_by_lookback = bool(best_trial.get("by_lookback"))
        has_combined_score = best_trial.get("combined_score") is not None

        if has_universe_hash and has_data_version and has_debug and has_by_lookback:
            result["checks"]["manifest_valid"] = True
            print("    âœ… Manifest ê²€ì¦ í†µê³¼")
            print(
                f"       - universe_hash: {manifest_dict['data']['universe_hash'][:16]}..."
            )
            print(f"       - data_version: {manifest_dict['data']['data_version']}")
            if has_debug:
                debug = best_trial.get("debug", {})
                print(
                    f"       - min_lookback_months: {debug.get('min_lookback_months')}"
                )
            if has_by_lookback:
                by_lb = best_trial.get("by_lookback", {})
                print(f"       - by_lookback: {list(by_lb.keys())} ({len(by_lb)}ê°œ)")
            if has_combined_score:
                print(
                    f"       - combined_score: {best_trial.get('combined_score'):.4f}"
                )
                print(
                    f"       - debug.lookback_start_date: {debug.get('lookback_start_date')}"
                )
        else:
            print(f"    âŒ Manifest ê²€ì¦ ì‹¤íŒ¨")
            print(f"       - universe_hash: {has_universe_hash}")
            print(f"       - data_version: {has_data_version}")
            print(f"       - debug: {has_debug}")

        print(f"    ì €ì¥: {filepath.name}")

        # telemetry_path ì €ì¥
        result["telemetry_path"] = str(get_telemetry().get_filepath())

        return result

    finally:
        if use_mock:
            runner_module._run_single_backtest = original_func


def run_phase15_loop(
    n_runs: int = 3,
    n_trials: int = 30,
    seed: int = 42,
    top_n: int = 5,
    use_mock: bool = True,
    analysis_mode: bool = False,
    stop_at_gate2: bool = False,
    force_gate2: bool = False,
) -> bool:
    """
    Phase 1.5 ë£¨í”„ ì‹¤í–‰

    Args:
        analysis_mode: Trueë©´ ê°€ë“œë ˆì¼ ì‹¤íŒ¨í•´ë„ manifest ì €ì¥ ë° ì‹¤íŒ¨ ì‚¬ìœ  ì§‘ê³„
        stop_at_gate2: Trueë©´ Gate2ê¹Œì§€ë§Œ ì‹¤í–‰ (Gate3 ê¸ˆì§€)
        force_gate2: Trueë©´ Gate1 í›„ë³´ 0ì¼ ë•Œ ê°€ë“œë ˆì¼ ë¬´ì‹œí•˜ê³  Gate2 ì‹¤í–‰ (analysis_mode í•„ìˆ˜)

    Returns:
        ëª¨ë“  runì´ í†µê³¼í–ˆìœ¼ë©´ True
    """
    # force_gate2ëŠ” analysis_modeì—ì„œë§Œ í—ˆìš©
    if force_gate2 and not analysis_mode:
        print("âš ï¸ --force-gate2ëŠ” --analysis-modeì™€ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.")
        force_gate2 = False

    print("\n" + "#" * 60)
    print("# Phase 1.5 ì‹¤ë°ì´í„° ë£¨í”„")
    print(f"# n_runs={n_runs}, n_trials={n_trials}, seed={seed}, top_n={top_n}")
    print(f"# TEST_MODE={is_test_mode()}, use_mock={use_mock}")
    if analysis_mode:
        print("# ANALYSIS_MODE=True (ê°€ë“œë ˆì¼ ì‹¤íŒ¨í•´ë„ manifest ì €ì¥)")
    if force_gate2:
        print("# FORCE_GATE2=True (Gate1 í›„ë³´ 0ì¼ ë•Œ ê°€ë“œë ˆì¼ ì™„í™”)")
    print("#" * 60)

    run_ids = ["A", "B", "C"][:n_runs]
    all_results = []

    for i, run_id in enumerate(run_ids):
        run_seed = seed + i * 100
        universe = UNIVERSES[run_id]
        period = PERIODS[run_id]

        result = run_single_phase15(
            run_id=run_id,
            universe=universe,
            period=period,
            n_trials=n_trials,
            seed=run_seed,
            top_n=top_n,
            use_mock=use_mock,
            analysis_mode=analysis_mode,
            force_gate2=force_gate2,
        )
        all_results.append(result)

    # ============================================================
    # ìµœì¢… ìš”ì•½ (Phase 1.7 í˜•ì‹)
    # ============================================================
    print("\n" + "=" * 60)
    print("Phase 1.7 ìµœì¢… ìš”ì•½")
    print("=" * 60)

    # Phase 1.7 í…Œì´ë¸” í˜•ì‹
    mode_str = "mock" if use_mock else "real"
    print(
        f"\n  {'Run':<4} {'Mode':<6} {'Preflight':<10} {'G1 Cand':<8} "
        f"{'G2 Ran':<7} {'G2 Pass':<8} {'Manifest':<12} {'Telemetry':<12}"
    )
    print("  " + "-" * 80)

    all_passed = True
    gate2_ran_count = 0
    for r in all_results:
        checks = r["checks"]
        pf = "âœ…" if checks["preflight_ok"] else "âŒ"
        g1_cand = r["gate1_passed"]
        g2_ran = "âœ…" if r.get("gate2_ran", False) else "âŒ"
        g2_pass = "âœ…" if checks["gate2_complete"] else "âŒ"
        mf = "âœ…" if r["manifest_path"] else "âŒ"
        tl = "âœ…" if r.get("telemetry_path") else "âŒ"

        if r.get("gate2_ran", False):
            gate2_ran_count += 1

        print(
            f"  {r['run_id']:<4} {mode_str:<6} {pf:<10} {g1_cand:<8} "
            f"{g2_ran:<7} {g2_pass:<8} {mf:<12} {tl:<12}"
        )

        if not all(checks.values()):
            all_passed = False

    # Phase 1.7 PASS ì¡°ê±´ ì²´í¬
    print("\n" + "-" * 60)
    print("Phase 1.7 PASS ì¡°ê±´ ì²´í¬:")

    # (A) 3íšŒ ì¤‘ 2íšŒ ì´ìƒì—ì„œ Gate2ê¹Œì§€ ì§„í–‰
    cond_a = gate2_ran_count >= min(2, n_runs)
    print(
        f"  (A) Gate2 ì§„í–‰: {gate2_ran_count}/{n_runs}íšŒ {'âœ… PASS' if cond_a else 'âŒ FAIL'}"
    )

    # (B) data_version=real_v1, requested_hash == effective_hash
    if not use_mock:
        cond_b = True  # real ëª¨ë“œì—ì„œëŠ” RUN_CONFIG ì´ë²¤íŠ¸ì—ì„œ í™•ì¸
        print(f"  (B) data_version=real_v1, hash ì¼ì¹˜: âœ… PASS (telemetry í™•ì¸)")
    else:
        cond_b = True  # mock ëª¨ë“œì—ì„œëŠ” í•´ë‹¹ ì—†ìŒ
        print(f"  (B) data_version=mock_v1 (mock ëª¨ë“œ): âœ… PASS")

    # (C) replay ì¬í˜„ì„± - ë³„ë„ ë„êµ¬ë¡œ í™•ì¸ í•„ìš”
    print(f"  (C) Replay ì¬í˜„ì„±: â³ tools/replay_manifest.pyë¡œ í™•ì¸ í•„ìš”")

    # ìµœì¢… íŒì •
    print("\n" + "-" * 60)
    if cond_a and cond_b:
        print(f"  ğŸ‰ Phase 1.7 ì¡°ê±´ (A)(B) PASS! (C)ëŠ” replay ë„êµ¬ë¡œ í™•ì¸")
    else:
        print(f"  âš ï¸ Phase 1.7 ì¡°ê±´ ë¯¸ì¶©ì¡±")

    # ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ì¶œë ¥
    print("\nìƒì„±ëœ íŒŒì¼:")
    for r in all_results:
        if r["manifest_path"]:
            print(f"  - Manifest: {r['manifest_path']}")
        if r.get("telemetry_path"):
            print(f"  - Telemetry: {r['telemetry_path']}")

    return all_passed


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Phase 1.5 ì‹¤ë°ì´í„° ë£¨í”„")
    parser.add_argument("--runs", type=int, default=3, help="ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 3)")
    parser.add_argument("--trials", type=int, default=30, help="ì‹œí–‰ íšŸìˆ˜ (ê¸°ë³¸: 30)")
    parser.add_argument("--seed", type=int, default=42, help="ëœë¤ ì‹œë“œ (ê¸°ë³¸: 42)")
    parser.add_argument("--top-n", type=int, default=5, help="Top-N (ê¸°ë³¸: 5)")
    parser.add_argument(
        "--real", action="store_true", help="ì‹¤ì œ ë°ì´í„° ì‚¬ìš© (ê¸°ë³¸: Mock)"
    )
    parser.add_argument(
        "--analysis-mode",
        action="store_true",
        help="ë¶„ì„ ëª¨ë“œ: ê°€ë“œë ˆì¼ ì‹¤íŒ¨í•´ë„ manifest ì €ì¥ ë° ì‹¤íŒ¨ ì‚¬ìœ  ì§‘ê³„",
    )
    parser.add_argument(
        "--stop-at-gate2",
        action="store_true",
        help="Gate2ê¹Œì§€ë§Œ ì‹¤í–‰ (Gate3 ê¸ˆì§€)",
    )
    parser.add_argument(
        "--force-gate2",
        action="store_true",
        help="Gate1 í›„ë³´ 0ì¼ ë•Œ ê°€ë“œë ˆì¼ ë¬´ì‹œí•˜ê³  Gate2 ì‹¤í–‰ (--analysis-mode í•„ìˆ˜, í…ŒìŠ¤íŠ¸ ì „ìš©)",
    )

    args = parser.parse_args()

    success = run_phase15_loop(
        n_runs=args.runs,
        n_trials=args.trials,
        seed=args.seed,
        top_n=args.top_n,
        use_mock=not args.real,
        analysis_mode=args.analysis_mode,
        stop_at_gate2=args.stop_at_gate2,
        force_gate2=args.force_gate2,
    )

    sys.exit(0 if success else 1)
